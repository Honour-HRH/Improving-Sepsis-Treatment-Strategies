{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorboardX import SummaryWriter\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('path/to/trainset')\n",
    "test_set = pd.read_csv('/path/to/testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# patient observations\n",
    "observ_cols = ['gender', 'age','elixhauser','re_admission', 'SOFA', 'SIRS', 'Weight_kg', 'GCS', 'HR',\n",
    "            'SysBP', 'MeanBP', 'DiaBP', 'RR', 'SpO2',\n",
    "            'Temp_C', 'FiO2_1', 'Potassium', 'Sodium', 'Chloride',\n",
    "            'Glucose', 'BUN', 'Creatinine', 'Magnesium', 'Calcium',\n",
    "            'Ionised_Ca', 'CO2_mEqL', 'SGOT', 'SGPT', 'Total_bili',\n",
    "            'Albumin', 'Hb', 'WBC_count', 'Platelets_count', 'PTT',\n",
    "            'PT', 'INR', 'Arterial_pH', 'paO2', 'paCO2',\n",
    "            'Arterial_BE', 'Arterial_lactate', 'HCO3', 'PaO2_FiO2',\n",
    "            'output_total', 'output_4hourly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class autoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \n",
    "        super(autoEncoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, dropout=0.2)\n",
    "        self.decoder = nn.LSTM(hidden_size, input_size, dropout=0.2)\n",
    "        \n",
    "        self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        self.encode_hidden = (Variable(torch.zeros(1, 1, self.hidden_size)), \\\n",
    "                              Variable(torch.zeros(1, 1, self.hidden_size)))\n",
    "        self.decode_hidden = (Variable(torch.zeros(1, 1, self.input_size)), \\\n",
    "                              Variable(torch.zeros(1, 1, self.input_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded, self.encode_hidden = self.encoder(x, self.encode_hidden)\n",
    "        repeat_last_encoded = encoded[-1].expand(x.size(0), 1, -1)\n",
    "        decoded, self.decode_hidden = self.decoder(repeat_last_encoded, self.decode_hidden)\n",
    "        return encoded.squeeze(1), decoded.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(train_set, test_set, autoencoder, lr=0.001, batch_size=128, num_epoch=50, print_every=10, val=False):\n",
    "    \n",
    "    uids = np.unique(train_set['icustayid'])\n",
    "    \n",
    "    enc_criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    train_uids, val_uids = train_test_split(uids, test_size=0.1, random_state=42)\n",
    "\n",
    "    for epoch in range(1, num_epoch+1):\n",
    "        \n",
    "        num_batches = train_uids.shape[0] // batch_size\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            enc_loss, enc_acc = 0, 0\n",
    "\n",
    "            batch_uids = train_uids[batch*batch_size: (batch+1)*batch_size]\n",
    "            batch_train_set = train_set[train_set['icustayid'].isin(batch_uids)]\n",
    "            \n",
    "            for i, uid in enumerate(batch_uids):\n",
    "\n",
    "                autoencoder.init_hidden()\n",
    "                patient = train_set[train_set['icustayid'] == uid]\n",
    "                enc_X = Variable(torch.FloatTensor(patient[observ_cols].values))\n",
    "                encoded, decoded = autoencoder(enc_X.unsqueeze(1))\n",
    "\n",
    "                enc_loss += enc_criterion(decoded, enc_X)\n",
    "                enc_acc += r2_score(enc_X.data.numpy(), decoded.squeeze(1).data.numpy(), \n",
    "                                    multioutput='variance_weighted')\n",
    "            \n",
    "            enc_loss /= batch_size\n",
    "            enc_acc /= batch_size\n",
    "            \n",
    "            if batch != 0 and batch % print_every == 0:\n",
    "                print ('epoch:{}/{}, batch:{}/{}, loss:{}, enc_acc:{}'.format(epoch, \n",
    "                                                                              num_epoch,batch, \\\n",
    "                                                                              num_batches, \\\n",
    "                                                                              enc_loss.data[0], \\\n",
    "                                                                              enc_acc))\n",
    "            optimizer.zero_grad()\n",
    "            enc_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if val:\n",
    "            print ('-----------------------')\n",
    "            print ('evaluating ...')\n",
    "            val_total_loss, val_enc_acc = do_eval(train_set, val_uids, autoencoder)\n",
    "            print ('Validating: loss:{}, enc_acc:{}'.format(val_total_loss.data[0], val_enc_acc))\n",
    "            print ('-----------------------')\n",
    "        \n",
    "        if epoch != 0 and epoch % 3 == 0:\n",
    "            print ('Testing ...')\n",
    "            do_eval(test_set, None, autoencoder)\n",
    "            print ('-----------------------')\n",
    "        \n",
    "        autoencoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(eval_set, eval_uids=None, autoencoder, output_embeddings=False):\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    enc_criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    if eval_uids is None:\n",
    "         eval_uids = np.unique(eval_set['icustayid'])\n",
    "\n",
    "    eval_enc_loss = 0\n",
    "    eval_enc_acc = 0\n",
    "    \n",
    "    embeddings = []\n",
    "\n",
    "    for uid in eval_uids:\n",
    "\n",
    "        autoencoder.init_hidden()\n",
    "        patient = eval_set[eval_set['icustayid'] == uid]\n",
    "        enc_X = Variable(torch.FloatTensor(patient[observ_cols].values))\n",
    "        encoded, decoded = autoencoder(enc_X.unsqueeze(1)) \n",
    "\n",
    "        eval_enc_loss += enc_criterion(decoded, enc_X)\n",
    "        eval_enc_acc += r2_score(enc_X.data.numpy(), decoded.squeeze(1).data.numpy(), \n",
    "                                    multioutput='variance_weighted')\n",
    "        if output_embeddings:\n",
    "            embeddings += encoded.data.numpy().tolist()\n",
    "        \n",
    "    eval_enc_acc /= eval_uids.shape[0]\n",
    "    \n",
    "    if output_embeddings:\n",
    "        return embeddings\n",
    "    \n",
    "    return eval_enc_loss, eval_enc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = autoEncoder(45, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_autoencoder(train_set, test_set, autoencoder, num_epoch=50, val=True) # 50 epoch, 0.94, 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(autoencoder, 'path/to/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_embeddings = do_eval(test_set, autoencoder, output_embeddings=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
